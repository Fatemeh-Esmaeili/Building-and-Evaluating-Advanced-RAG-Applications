{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21f9146a-c6f3-4a78-b3fb-0d262492e87c",
   "metadata": {},
   "source": [
    "# Lesson 3: Sentence Window Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660a5cd8",
   "metadata": {
    "height": 249
   },
   "source": [
    "In *sentence-window retrieval*, we:\n",
    "\n",
    "Break a document into small sentences,\n",
    "\n",
    " - Save a \"context window\" (a few surrounding sentences) in metadata,\n",
    "\n",
    " - Retrieve the most relevant sentence chunks,\n",
    "\n",
    " - **Replace** the sentence with its larger context window using postproc,\n",
    "\n",
    " - **Rerank** those chunks using a cross-encoder model with rerank,\n",
    "\n",
    " - And finally, **send the top few to the LLM** for a well-grounded response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faea7c2c",
   "metadata": {
    "height": 30
   },
   "source": [
    "This notebook demonstrates an advanced sentence-window RAG pipeline for document question-answering:\n",
    "\n",
    "1- PDF Loading: Uses SimpleDirectoryReader to extract text.\n",
    "\n",
    "2- Sentence Windowing: Splits text into sentences with surrounding context (window_size=3).\n",
    "\n",
    "3- Indexing: Builds a semantic vector index of these contextual nodes using BAAI/bge-small-en-v1.5.\n",
    "\n",
    "4- Postprocessing:\n",
    "\n",
    "  - MetadataReplacementPostProcessor: Injects sentence windows as context.\n",
    "\n",
    "  - SentenceTransformerRerank: Selects top relevant chunks using BAAI’s reranker.\n",
    "\n",
    "5- Query Engine: Combines these steps into a retriever that feeds optimized chunks to GPT-3.5.\n",
    "\n",
    "6- Evaluation: Runs TruLens evaluations for different window sizes (1, 3, 5) using recorded responses to a set of predefined questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541cadae-916c-42da-93ff-75d7f788ee8d",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78f48098-16d8-4209-b722-1ec6a0220c96",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "import utils2\n",
    "\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = utils2.get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee55360-1fc4-41cc-bd49-82027797ea40",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./bioengineering-2247773.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a12ada81-5c1c-47c9-b7b4-ba621a80bbcd",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(type(documents), \"\\n\")\n",
    "print(len(documents), \"\\n\")\n",
    "print(type(documents[0]))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f662e4-3fb8-40c6-acc5-e8510348d113",
   "metadata": {
    "height": 96,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge into single large document rather than one document per-page\n",
    "from llama_index import Document\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff01ea-b5b0-4e65-8565-b2444812bd84",
   "metadata": {},
   "source": [
    "## Window-sentence retrieval setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a194b93-f975-4d38-babe-218d3aae6117",
   "metadata": {
    "height": 283,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# window size of three and top-k value of 6\n",
    "\n",
    "# SentenceWindowNodeParser is an Object that will split a document \n",
    "# into individual sentences and then augment each sentence chunk\n",
    "# with the surronding contexts around that sentence\n",
    "\n",
    "\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "# create the sentence window node parser w/ default settings\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88973de6-be8f-4653-aca3-8d0e884d9470",
   "metadata": {
    "height": 113,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Demonstrate how NodeParser works with small sample\n",
    "\n",
    "text = \"hello. how are you? I am fine! Thank you! It is sunny and warm today.\"\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents([Document(text=text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "860e40a8-6f50-4345-b314-c4d9349681c6",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print([x.text for x in nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4334e02-c423-4555-8446-4794eadccd0a",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Window size is 3\n",
    "\n",
    "print(nodes[1].metadata[\"window\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03fea59c-ed0f-4cc8-87b4-871798edb094",
   "metadata": {
    "height": 79,
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"hello. foo bar. cat dog. mouse\"\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents([Document(text=text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c329808d-248f-422e-bce0-1ac1ecba79a5",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print([x.text for x in nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfd1eccb-823d-4f6f-8d6c-a9064dc76922",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(nodes[0].metadata[\"window\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea97dda-8457-4f32-a2c7-26ae92eaf0b4",
   "metadata": {},
   "source": [
    "### Building the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb2387dd-6bdb-4d0d-98ea-2b468ddfe160",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5102d65b-3584-4a25-88be-7d5dbc70c678",
   "metadata": {
    "height": 183,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# embeded model from HuggingFace\n",
    "\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "sentence_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    # embed_model=\"local:BAAI/bge-large-en-v1.5\"\n",
    "    node_parser=node_parser,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbf15398-25e9-47bd-b840-e57272d38683",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "sentence_index = VectorStoreIndex.from_documents(\n",
    "    [document], service_context=sentence_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55497194-35c9-4f91-85c4-ef6adb1aff78",
   "metadata": {
    "height": 147,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the index and its associated storage \n",
    "# (documents, embeddings, metadata, etc.) to disk \n",
    "# so you can reload (no need to rebuild) it later \n",
    "# without recomputing.\n",
    "\n",
    "sentence_index.storage_context.persist(persist_dir=\"./sentence_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba4e7371-7a8e-451c-a5e7-e9e3d7371614",
   "metadata": {
    "height": 351,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This block of code is optional to check\n",
    "# if an index file exist, then it will load it\n",
    "# if not, it will rebuild it\n",
    "\n",
    "import os\n",
    "from llama_index import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index import load_index_from_storage\n",
    "\n",
    "if not os.path.exists(\"./sentence_index\"):\n",
    "    sentence_index = VectorStoreIndex.from_documents(\n",
    "        [document], service_context=sentence_context\n",
    "    )\n",
    "\n",
    "    sentence_index.storage_context.persist(persist_dir=\"./sentence_index\")\n",
    "else:\n",
    "    sentence_index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=\"./sentence_index\"),\n",
    "        service_context=sentence_context\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b934879-e2c3-44f8-b98c-0300dc6389a9",
   "metadata": {},
   "source": [
    "### Building the postprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35698ff1",
   "metadata": {
    "height": 164
   },
   "source": [
    "**Why use this?**\n",
    "\n",
    "In advanced RAG techniques like sentence-window retrieval, you may:\n",
    "\n",
    " - Split your documents into sentences (e.g., \"query sentence\"), but\n",
    "\n",
    " - Attach surrounding context (e.g., the \"sentence window\") as metadata.\n",
    "\n",
    "Then, when retrieving chunks based on the sentence, you don't want just the sentence — you want the full context window shown to the model. So this postprocessor swaps the sentence with the richer context stored in metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f22f435-a90a-447c-9e63-8aebec1e968d",
   "metadata": {
    "height": 164,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replaces the retrieved document text with metadata from \n",
    "# a specific field — in this case, the \"window\" field.\n",
    "\n",
    "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "postproc = MetadataReplacementPostProcessor(\n",
    "    target_metadata_key=\"window\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77fad243-7ea8-4a81-b85e-91c3e638d932",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test postprocessor\n",
    "\n",
    "from llama_index.schema import NodeWithScore\n",
    "from copy import deepcopy\n",
    "\n",
    "scored_nodes = [NodeWithScore(node=x, score=1.0) for x in nodes]\n",
    "nodes_old = [deepcopy(n) for n in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "946c4219-1d6e-4717-8c8f-40db659ea517",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodes_old[2].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89900cb4-5d03-43f6-9d1d-de1350bfd299",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "replaced_nodes = postproc.postprocess_nodes(scored_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72f98170-e368-461b-9939-5da80c4940e4",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(replaced_nodes[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aa4a50-5c0b-4f23-9eae-ec67cb701e29",
   "metadata": {},
   "source": [
    "### Adding a reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d0f5a7",
   "metadata": {
    "height": 45
   },
   "source": [
    "Create a reranker that uses a pretrained model to find and keep only the top 2 most relevant text chunks from retrieved documents, improving LLM answer quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe747c5",
   "metadata": {
    "height": 538
   },
   "source": [
    "🧠 **What it does:**\n",
    "\n",
    "SentenceTransformerRerank:\n",
    "This is a postprocessor that uses a pre-trained transformer model (from Hugging Face) to re-rank retrieved document chunks (nodes) based on how well they match the user's question.\n",
    "\n",
    "top_n=2:\n",
    "After re-ranking all the results, it keeps only the top 2 most relevant chunks.\n",
    "\n",
    "model=\"BAAI/bge-reranker-base\":\n",
    "It uses the BAAI/bge-reranker-base model — a powerful cross-encoder trained to score how well a document matches a query.\n",
    "It's more accurate than typical embedding-based similarity because it directly compares the query and passage together.\n",
    "\n",
    "🏗️ **What happens under the hood:**\n",
    "The retriever gives you, say, 10 candidate chunks (Nodes).\n",
    "\n",
    "The SentenceTransformerRerank:\n",
    "\n",
    "Pairs each chunk with the query.\n",
    "\n",
    "Sends each (query, chunk) pair into the reranker model.\n",
    "\n",
    "Gets a relevance score for each.\n",
    "\n",
    "It sorts by score and keeps only the top 2 chunks.\n",
    "\n",
    "✅ **Why use it?**\n",
    "Improves accuracy: It often filters out less relevant chunks retrieved by simpler methods (like vector search).\n",
    "\n",
    "Helpful for precise answers: Especially useful when your query needs high-quality grounding from the most relevant chunks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57608e01-ee96-4619-aab1-81d7fce1cbd1",
   "metadata": {
    "height": 147,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "# BAAI/bge-reranker-base\n",
    "# link: https://huggingface.co/BAAI/bge-reranker-base\n",
    "rerank = SentenceTransformerRerank(\n",
    "    top_n=2, model=\"BAAI/bge-reranker-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d89da07-6b0c-477f-bd0a-2f466c9b159b",
   "metadata": {
    "height": 215,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a query and mock search\n",
    "\n",
    "from llama_index import QueryBundle\n",
    "from llama_index.schema import TextNode, NodeWithScore\n",
    "\n",
    "query = QueryBundle(\"I want a dog.\")\n",
    "\n",
    "scored_nodes = [\n",
    "    NodeWithScore(node=TextNode(text=\"This is a cat\"), score=0.6),\n",
    "    NodeWithScore(node=TextNode(text=\"This is a dog\"), score=0.4),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1adc6cb4-e741-480a-ab13-06e9194b13b2",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "reranked_nodes = rerank.postprocess_nodes(\n",
    "    scored_nodes, query_bundle=query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3be841f6-0ab2-4c60-b52f-d27f81fcb1bb",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print([(x.text, x.score) for x in reranked_nodes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d51921-4b0b-4d89-963f-9a9e0ea439d9",
   "metadata": {},
   "source": [
    "### Runing the query engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f1279",
   "metadata": {
    "height": 166
   },
   "source": [
    "This line builds a smart query engine that:\n",
    "\n",
    "Retrieves the top 6 similar text chunks,\n",
    "\n",
    "Replaces them with richer context (from metadata),\n",
    "\n",
    "Reranks them by true relevance using a powerful model, and\n",
    "\n",
    "Feeds the best ones to the LLM to generate better answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47bfe5a5-5ce1-4baa-ba61-3b39d16a2337",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence_window_engine = sentence_index.as_query_engine(\n",
    "    similarity_top_k=6, node_postprocessors=[postproc, rerank]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4f166b9-99f9-4507-af59-3347eaf596c6",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "window_response = sentence_window_engine.query(\n",
    "    \"What is the contribution of this article?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ada6304b-a86c-4f6c-a2e9-8ac5b1091a58",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from llama_index.response.notebook_utils import display_response\n",
    "\n",
    "display_response(window_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcbe614-befe-4bf7-9813-2c91899650e9",
   "metadata": {},
   "source": [
    "## Putting it all Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "313e8b1a-e9e7-4141-a4d2-72fb31a7e057",
   "metadata": {
    "height": 929
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index import ServiceContext, VectorStoreIndex, StorageContext\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index import load_index_from_storage\n",
    "\n",
    "\n",
    "def build_sentence_window_index(\n",
    "    documents,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    sentence_window_size=3,\n",
    "    save_dir=\"sentence_index\",\n",
    "):\n",
    "    # create the sentence window node parser w/ default settings\n",
    "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=sentence_window_size,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "    )\n",
    "    sentence_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        node_parser=node_parser,\n",
    "    )\n",
    "    if not os.path.exists(save_dir):\n",
    "        sentence_index = VectorStoreIndex.from_documents(\n",
    "            documents, service_context=sentence_context\n",
    "        )\n",
    "        sentence_index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        sentence_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=save_dir),\n",
    "            service_context=sentence_context,\n",
    "        )\n",
    "\n",
    "    return sentence_index\n",
    "\n",
    "\n",
    "def get_sentence_window_query_engine(\n",
    "    sentence_index, similarity_top_k=6, rerank_top_n=2\n",
    "):\n",
    "    # define postprocessors\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "\n",
    "    sentence_window_engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]\n",
    "    )\n",
    "    return sentence_window_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b5bb36f-97f9-4e03-bd4e-3ceb635a868b",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "\n",
    "index = build_sentence_window_index(\n",
    "    [document],\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n",
    "    save_dir=\"./sentence_index\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d0e78e0-7765-4037-a5fa-63b711dab5aa",
   "metadata": {
    "height": 62
   },
   "outputs": [],
   "source": [
    "query_engine = get_sentence_window_query_engine(index, similarity_top_k=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb855430-8fcb-4c25-8d83-a30160449acf",
   "metadata": {},
   "source": [
    "## TruLens Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07aa5320-5c1c-4636-aa72-e6a786a58c8a",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "eval_questions = []\n",
    "with open('eval_questions.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Remove newline character and convert to integer\n",
    "        item = line.strip()\n",
    "        #print(item)\n",
    "        eval_questions.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f9bdeb7-8c37-46b0-80f2-eb8ef1cc7b59",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "\n",
    "def run_evals(eval_questions, tru_recorder, query_engine):\n",
    "    for question in eval_questions:\n",
    "        with tru_recorder as recording:\n",
    "            response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7818a9b-04cc-4a6e-98ae-d3d0ab309998",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "from utils2 import get_prebuilt_trulens_recorder\n",
    "\n",
    "from trulens_eval import Tru\n",
    "\n",
    "Tru().reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ef4d27-2c4e-4dd0-85ed-a69a0f8eea00",
   "metadata": {},
   "source": [
    "### Sentence window size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93e6c651-838d-4c36-b806-efd0d8c2d5f0",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "sentence_index_1 = build_sentence_window_index(\n",
    "    documents,\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    sentence_window_size=1,\n",
    "    save_dir=\"sentence_index_1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8dc445f4-0311-4192-ac2e-4ea94b2653e5",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "sentence_window_engine_1 = get_sentence_window_query_engine(\n",
    "    sentence_index_1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2750e2b1-ca60-495f-8168-33520740916c",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "tru_recorder_1 = get_prebuilt_trulens_recorder(\n",
    "    sentence_window_engine_1,\n",
    "    app_id='sentence window engine 1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca942b16-52be-4107-861c-33daeca1f619",
   "metadata": {
    "height": 45
   },
   "outputs": [],
   "source": [
    "run_evals(eval_questions, tru_recorder_1, sentence_window_engine_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55dc7629-7fb7-4801-bd52-c9ab0da50267",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "Tru().run_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d36ca4-8f52-4510-bfab-aa00b704d179",
   "metadata": {},
   "source": [
    "### Note about the dataset of questions\n",
    "- Since this evaluation process takes a long time to run, the following file `generated_questions.text` contains one question (the one mentioned in the lecture video).\n",
    "- If you would like to explore other possible questions, feel free to explore the file directory by clicking on the \"Jupyter\" logo at the top right of this notebook. You'll see the following `.text` files:\n",
    "\n",
    "> - `generated_questions_01_05.text`\n",
    "> - `generated_questions_06_10.text`\n",
    "> - `generated_questions_11_15.text`\n",
    "> - `generated_questions_16_20.text`\n",
    "> - `generated_questions_21_24.text`\n",
    "\n",
    "Note that running an evaluation on more than one question can take some time, so we recommend choosing one of these files (with 5 questions each) to run and explore the results.\n",
    "\n",
    "- For evaluating a personal project, an eval set of 20 is reasonable.\n",
    "- For evaluating business applications, you may need a set of 100+ in order to cover all the use cases thoroughly.\n",
    "- Note that since API calls can sometimes fail, you may occasionally see null responses, and would want to re-run your evaluations.  So running your evaluations in smaller batches can also help you save time and cost by only re-running the evaluation on the batches with issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c55e4e1-5f1f-4c50-bd8e-4b54299377da",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "eval_questions = []\n",
    "with open('eval_questions.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Remove newline character and convert to integer\n",
    "        item = line.strip()\n",
    "        eval_questions.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c9977-c606-41bb-b87a-ec3cc17eabaf",
   "metadata": {},
   "source": [
    "### Sentence window size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0c8bea9d-9805-4302-b8c0-6e0f3eeec030",
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "sentence_index_3 = build_sentence_window_index(\n",
    "    documents,\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    sentence_window_size=3,\n",
    "    save_dir=\"sentence_index_3\",\n",
    ")\n",
    "sentence_window_engine_3 = get_sentence_window_query_engine(\n",
    "    sentence_index_3\n",
    ")\n",
    "\n",
    "tru_recorder_3 = get_prebuilt_trulens_recorder(\n",
    "    sentence_window_engine_3,\n",
    "    app_id='sentence window engine 3'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e69d459e-e984-46b4-8fd9-ca4f77150647",
   "metadata": {
    "height": 45
   },
   "outputs": [],
   "source": [
    "run_evals(eval_questions, tru_recorder_3, sentence_window_engine_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dfdfb770",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "Tru().run_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbbc8a5",
   "metadata": {
    "height": 30
   },
   "source": [
    "### Sentence window size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "685accf9",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "eval_questions = []\n",
    "with open('eval_questions.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Remove newline character and convert to integer\n",
    "        item = line.strip()\n",
    "        eval_questions.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "836e2b8d",
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "sentence_index_5 = build_sentence_window_index(\n",
    "    documents,\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    sentence_window_size=3,\n",
    "    save_dir=\"sentence_index_5\",\n",
    ")\n",
    "sentence_window_engine_5 = get_sentence_window_query_engine(\n",
    "    sentence_index_5\n",
    ")\n",
    "\n",
    "tru_recorder_5 = get_prebuilt_trulens_recorder(\n",
    "    sentence_window_engine_5,\n",
    "    app_id='sentence window engine 5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb02ab19",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "run_evals(eval_questions, tru_recorder_5, sentence_window_engine_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9df9248e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "Tru().run_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f8146",
   "metadata": {
    "height": 419
   },
   "source": [
    "| Code / Function                                                  | Purpose                                                                    |\n",
    "| ---------------------------------------------------------------- | -------------------------------------------------------------------------- |\n",
    "| `SimpleDirectoryReader(input_files=[...])`                       | Loads PDF into a list of `Document` objects.                               |\n",
    "| `Document(text=\"...\")`                                           | Merges multiple document chunks into one continuous text document.         |\n",
    "| `SentenceWindowNodeParser.from_defaults(...)`                    | Splits text into sentence chunks with surrounding context (“window”).      |\n",
    "| `node_parser.get_nodes_from_documents([...])`                    | Applies the parser to produce context-augmented nodes.                     |\n",
    "| `OpenAI(model=\"gpt-3.5-turbo\")`                                  | Sets up LLM interface with OpenAI's GPT model.                             |\n",
    "| `ServiceContext.from_defaults(...)`                              | Bundles LLM, embedding model, and node parser into a context object.       |\n",
    "| `VectorStoreIndex.from_documents([...])`                         | Builds a vector-based index of sentence nodes.                             |\n",
    "| `sentence_index.storage_context.persist(...)`                    | Saves the index and embeddings to disk.                                    |\n",
    "| `load_index_from_storage(...)`                                   | Loads a saved index from disk without recomputation.                       |\n",
    "| `MetadataReplacementPostProcessor(target_metadata_key=\"window\")` | Replaces raw sentence text with full windowed context for final LLM input. |\n",
    "| `SentenceTransformerRerank(top_n=2, model=...)`                  | Re-ranks top `k` retrieved nodes using a cross-encoder for accuracy.       |\n",
    "| `NodeWithScore(node=x, score=...)`                               | Attaches a relevance score to nodes (used by reranker/postprocessor).      |\n",
    "| `deepcopy(n)`                                                    | Creates deep copies of nodes to avoid modifying originals.                 |\n",
    "| `sentence_index.as_query_engine(...)`                            | Constructs a query engine with retrieval + postprocessors.                 |\n",
    "| `display_response(...)`                                          | Neatly formats and shows the LLM’s final response.                         |\n",
    "| `build_sentence_window_index(...)`                               | Reusable function to create and store index with custom settings.          |\n",
    "| `get_sentence_window_query_engine(...)`                          | Returns a query engine with metadata replacement and reranking.            |\n",
    "| `run_evals(eval_questions, tru_recorder, query_engine)`          | Runs a batch of evaluation questions against the query engine.             |\n",
    "| `get_prebuilt_trulens_recorder(...)`                             | Sets up a recorder to track evaluation metrics with TruLens.               |\n",
    "| `Tru().run_dashboard()`                                          | Launches the TruLens evaluation dashboard.                                 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
